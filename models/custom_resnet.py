# -*- coding: utf-8 -*-
"""custom_resnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17pV0gfAvm_KsX1d9YtnLw2K5Ipn4abiJ
"""

### Custom Resnet Model

import torch
import torch.nn as nn
import torch.nn.functional as F

drop = 0.05

class EnterBlock(nn.Module):
  """
  Block to support Conv 3x3 (s1, p1) >> MaxPool2D >> BN >> RELU 
  """
  def __init__(self, num_in_channels, num_out_channels):
    super(EnterBlock,self).__init__()
    self.block = nn.Sequential(
        nn.Conv2d(in_channels = num_in_channels, out_channels = num_out_channels, kernel_size = 3, stride = 1, padding = 1),
        nn.MaxPool2d(kernel_size = 2, stride = 2),
        nn.BatchNorm2d(num_out_channels),
        nn.ReLU()
    )

  def forward (self,x):
    return self.block(x)

class BasicBlock(nn.Module):
  """
  Basic Block of resnet (Conv-BN-ReLU-Conv-BN-ReLU)

  """
  def __init__(self,num_in_channels, num_out_channels, stride = 1):
    super(BasicBlock, self).__init__()
    self.res_block = nn.Sequential(
        nn.Conv2d(in_channels=num_in_channels, out_channels = num_out_channels, kernel_size = 3, stride= stride, padding = 1, bias = False),
        nn.BatchNorm2d(num_out_channels),
        nn.ReLU(),
        nn.Conv2d(in_channels=num_out_channels, out_channels = num_out_channels, kernel_size = 3, stride= stride, padding = 1, bias = False),
        nn.BatchNorm2d(num_out_channels),
        nn.ReLU()
    )

  def forward(self,x):
    return self.res_block(x)


class CustomResnet(nn.Module):
  """
  Custom Resnet Model
  PrepLayer - 
    Conv 3x3 s1, p1) >> BN >> RELU [64k]
  Layer1 -
    X = Conv 3x3 (s1, p1) >> MaxPool2D >> BN >> RELU [128k]
    R1 = ResBlock( (Conv-BN-ReLU-Conv-BN-ReLU))(X) [128k] 
    Add(X, R1)
  Layer 2 -
    Conv 3x3 [256k]
    MaxPooling2D
    BN
    ReLU
  Layer 3 -
    X = Conv 3x3 (s1, p1) >> MaxPool2D >> BN >> RELU [512k]
    R2 = ResBlock( (Conv-BN-ReLU-Conv-BN-ReLU))(X) [512k]
    Add(X, R2)
  MaxPooling with Kernel Size 4
  FC Layer 
  SoftMax
  """
  def __init__(self, num_class = 10, drop = 0.05):
    super(CustomResnet,self).__init__()

    ## Prep Layer
    self.prep = nn.Sequential(
        nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, stride = 1, padding =1, bias = False),
        nn.BatchNorm2d(64),
        nn.ReLU(),
        nn.Dropout(drop),
    )

    # Layer 1
    self.layer_1_entry = EnterBlock(num_in_channels = 64, num_out_channels = 128)
    self.resnet_block_1 = BasicBlock(num_in_channels = 128, num_out_channels = 128, stride = 1)

    # Layer 2
    self.layer2 = EnterBlock(num_in_channels = 128, num_out_channels = 256)

    # Layer 3 
    self.layer_3_entry = EnterBlock(num_in_channels = 256, num_out_channels = 512)
    self.resnet_block_2 = BasicBlock(num_in_channels = 512, num_out_channels = 512, stride = 1)
    
    # max pool
    self.maxpool = nn.MaxPool2d(kernel_size = 4, stride = 1)

    # FC layer
    self.fc = nn.Linear(in_features = 512, out_features = num_class, bias = False)

  def forward(self, x):
    x = self.prep(x)

    x = self.layer_1_entry(x)
    res1 = self.resnet_block_1(x)
    x = x + res1


    x = self.layer2(x)

    x = self.layer_3_entry(x)
    res2 = self.resnet_block_2(x)
    x = x + res2

    x = self.maxpool(x)

    x = x.view(-1, 512)
    x = self.fc(x)

    x = F.log_softmax(x, dim=-1)

    return x

# from torchsummary import summary

# use_cuda = torch.cuda.is_available()
# device = torch.device("cuda" if use_cuda else "cpu")
# print(device)
# net = CustomResnet(drop=0.0).to(device)
# summary(net, input_size=(3, 32, 32))