# -*- coding: utf-8 -*-
"""Basic_Transformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1en7aksqJU9dq0e_k4LLxym8veHXwq3Yc
"""

from numpy import matmul
import torch
import torch.nn as nn
import torch.nn.functional as F


class ULTIMUS(nn.Module):
  def __init__(self):
    super(ULTIMUS,self).__init__()
    self.lin1 = nn.Sequential(
        nn.Linear(in_features = 48, out_features = 8, bias = False),
        nn.ReLU()
    )
    self.lin2 = nn.Sequential(
        nn.Linear(in_features = 8, out_features = 48, bias = False),
        nn.ReLU()
    )
    
  def forward(self,x):    
    K = self.lin1(x) # size 8 x1
    # print(K.shape)
    Q = self.lin1(x) # size 8 x1
    QT = torch.transpose(Q,0,1) # size 1x8
    V = self.lin1(x) # size 8
    QTK = torch.matmul(QT,K)
    # print(QTK.shape)
    AM = F.softmax((QTK / pow(8.0,0.5)),dim =-1)
    # print(AM.shape)
    Z = torch.matmul(V,AM)
    # print(Z.shape)
    out = self.lin2(Z)
    # print(out.shape)
    return out


# Basic Transformer Model

class BasicTransformer(nn.Module):
    def __init__(self):
      super(BasicTransformer,self).__init__()

      #Embedding block, creates embeding for input image
      self.embed = nn.Sequential(
          nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3,stride=1, bias = False), # output size 30x30X16
          nn.BatchNorm2d(16),
          nn.ReLU(),
          nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3,stride=1, bias = False),# size 28x28X32
          nn.BatchNorm2d(32),
          nn.ReLU(),
          nn.Conv2d(in_channels = 32, out_channels = 48, kernel_size = 3,stride=1, bias = False), # size 26x26X48
          nn.AvgPool2d(26)
      )

      ## Transformer Block
      self.ulitmus = ULTIMUS()

      ## Feed Forward Connection
      self.FFC = nn.Linear(in_features = 48, out_features = 10, bias = False)

    def forward(self,x):
      x= self.embed(x)
      # print(x.shape)
      x = x.view(-1,48)
      # print(x.shape)

      x = self.ulitmus(x)
      x = self.ulitmus(x)
      x = self.ulitmus(x)
      x = self.ulitmus(x)
      
      x = self.FFC(x)

      return x

# from torchsummary import summary
# use_cuda = torch.cuda.is_available()
# device = torch.device("cuda" if use_cuda else "cpu")
# print(device)
# net = BasicTransformer().to(device)
# summary(net, input_size=(3, 32, 32))

